{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QTCR-Net: Quantum Temporal Convolutional Reservoir Network\n",
    "\n",
    "## End-to-End Demo for DVS128 Event Camera Classification\n",
    "\n",
    "This notebook demonstrates the complete workflow for training and evaluating QTCR-Net, a novel quantum-hybrid neural network architecture for event-based vision.\n",
    "\n",
    "**Architecture Highlights:**\n",
    "- Fully convolutional spatio-temporal feature extraction (NO MLP encoders)\n",
    "- Temporal Convolutional Network (TCN) with multi-scale dilations\n",
    "- Multiple quantum temporal reservoirs (PennyLane QNodes)\n",
    "- Dual-head classification (waveform + voltage)\n",
    "- Hybrid quantum-classical causal temporal modeling\n",
    "\n",
    "**Author:** QTCR-Net Research Team  \n",
    "**Date:** 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import torch\n",
    "import pennylane as qml\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"\\nExperiment: {config['experiment']['name']}\")\n",
    "print(f\"Description: {config['experiment']['description']}\")\n",
    "print(f\"\\nData Configuration:\")\n",
    "print(f\"  Window duration: {config['data']['window']['duration_sec']}s\")\n",
    "print(f\"  Temporal bins: {config['data']['window']['temporal_bins']}\")\n",
    "print(f\"  Spatial patch size: {config['data']['spatial']['patch_size']}x{config['data']['spatial']['patch_size']}\")\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Quantum groups: {config['model']['quantum_reservoir']['num_groups']}\")\n",
    "print(f\"  Qubits per group: {config['model']['quantum_reservoir']['qubits_per_group']}\")\n",
    "print(f\"  Quantum layers: {config['model']['quantum_reservoir']['circuit']['num_layers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Convert raw CSV event streams to voxel grid representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if preprocessing is needed\n",
    "manifest_path = Path(config['data']['manifest_path'])\n",
    "\n",
    "if not manifest_path.exists():\n",
    "    print(\"Manifest not found. Running preprocessing...\")\n",
    "    print(\"\\nNote: This may take a while for large datasets.\")\n",
    "    print(\"For testing, you can limit the number of files/windows in preprocess.py\\n\")\n",
    "    \n",
    "    # Run preprocessing\n",
    "    !python preprocess.py --config config.yaml\n",
    "else:\n",
    "    print(f\"Manifest already exists at: {manifest_path}\")\n",
    "    print(\"Skipping preprocessing. To reprocess, delete the manifest file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect manifest\n",
    "manifest = pd.read_csv(manifest_path)\n",
    "\n",
    "print(f\"Total samples: {len(manifest)}\")\n",
    "print(f\"\\nFirst few samples:\")\n",
    "display(manifest.head())\n",
    "\n",
    "print(f\"\\nLabel distributions:\")\n",
    "print(\"\\nWaveform:\")\n",
    "print(manifest['waveform_label'].value_counts())\n",
    "print(\"\\nVoltage:\")\n",
    "print(manifest['voltage_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Voxel Grids\n",
    "\n",
    "Load and visualize some example voxel grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_voxel_grid(voxel_path, title=\"Voxel Grid\"):\n",
    "    \"\"\"\n",
    "    Visualize a voxel grid from a .npy file.\n",
    "    \n",
    "    Args:\n",
    "        voxel_path: Path to .npy file\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    voxel = np.load(voxel_path)  # [C, T, H, W]\n",
    "    C, T, H, W = voxel.shape\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    # Show 4 time slices for each polarity\n",
    "    time_indices = np.linspace(0, T-1, 4, dtype=int)\n",
    "    \n",
    "    for i, t_idx in enumerate(time_indices):\n",
    "        # ON events (polarity 0)\n",
    "        axes[0, i].imshow(voxel[0, t_idx], cmap='hot', interpolation='nearest')\n",
    "        axes[0, i].set_title(f'ON Events - t={t_idx}/{T}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # OFF events (polarity 1)\n",
    "        axes[1, i].imshow(voxel[1, t_idx], cmap='cool', interpolation='nearest')\n",
    "        axes[1, i].set_title(f'OFF Events - t={t_idx}/{T}')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Voxel shape: {voxel.shape}\")\n",
    "    print(f\"Value range: [{voxel.min():.3f}, {voxel.max():.3f}]\")\n",
    "    print(f\"Non-zero elements: {(voxel != 0).sum()} / {voxel.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few random samples\n",
    "num_samples_to_viz = 2\n",
    "sample_indices = np.random.choice(len(manifest), num_samples_to_viz, replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    sample = manifest.iloc[idx]\n",
    "    voxel_path = sample['npy_path']\n",
    "    waveform = sample['waveform_label']\n",
    "    voltage = sample['voltage_label']\n",
    "    \n",
    "    title = f\"Waveform: {waveform}, Voltage: {voltage}\"\n",
    "    visualize_voxel_grid(voxel_path, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import create_dataloaders\n",
    "\n",
    "# Create data loaders\n",
    "train_loader, val_loader, test_loader = create_dataloaders(config, train_augment=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a batch\n",
    "batch_voxels, batch_labels = next(iter(train_loader))\n",
    "\n",
    "print(f\"Batch voxel shape: {batch_voxels.shape}\")\n",
    "print(f\"Batch voxel dtype: {batch_voxels.dtype}\")\n",
    "print(f\"Waveform labels: {batch_labels['waveform']}\")\n",
    "print(f\"Voltage labels: {batch_labels['voltage']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build QTCR-Net Model\n",
    "\n",
    "Construct the quantum-hybrid architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qtcr_model import QTCRNet\n",
    "\n",
    "# Create model\n",
    "model = QTCRNet(config).to(device)\n",
    "\n",
    "# Print model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Memory (approx): {total_params * 4 / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    test_input = batch_voxels[:2].to(device)  # Test with 2 samples\n",
    "    waveform_logits, voltage_logits = model(test_input)\n",
    "\n",
    "print(f\"Test input shape: {test_input.shape}\")\n",
    "print(f\"Waveform logits shape: {waveform_logits.shape}\")\n",
    "print(f\"Voltage logits shape: {voltage_logits.shape}\")\n",
    "print(\"\\nForward pass successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Quantum Circuit\n",
    "\n",
    "Visualize one of the quantum reservoir circuits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the first quantum reservoir\n",
    "quantum_layer = model.quantum_layer\n",
    "first_reservoir = quantum_layer.reservoirs[0]\n",
    "\n",
    "print(f\"First Quantum Reservoir:\")\n",
    "print(f\"  Qubits: {first_reservoir.num_qubits}\")\n",
    "print(f\"  Layers: {first_reservoir.num_layers}\")\n",
    "print(f\"  Entanglement: {first_reservoir.entanglement}\")\n",
    "print(f\"  Trainable: {first_reservoir.trainable}\")\n",
    "\n",
    "# Draw circuit (requires pennylane[matplotlib])\n",
    "try:\n",
    "    # Create dummy input\n",
    "    dummy_features = torch.randn(first_reservoir.num_qubits)\n",
    "    dummy_params = first_reservoir.quantum_params.detach()\n",
    "    \n",
    "    # Draw circuit\n",
    "    fig, ax = qml.draw_mpl(first_reservoir.qnode)(dummy_features, dummy_params)\n",
    "    plt.title(\"Quantum Reservoir Circuit (Single Reservoir)\", fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not draw circuit: {e}\")\n",
    "    print(\"Install pennylane[matplotlib] for circuit visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train QTCR-Net\n",
    "\n",
    "Train the model with dual-head losses, AMP, and early stopping.\n",
    "\n",
    "**Note:** For a full training run, use `train.py` from the command line. This notebook shows a short training demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import QTCRNetTrainer\n",
    "\n",
    "# Option 1: Train in notebook (short demo)\n",
    "# Reduce epochs for quick demo\n",
    "config_demo = config.copy()\n",
    "config_demo['training']['num_epochs'] = 5  # Short demo\n",
    "config_demo['training']['early_stopping']['enabled'] = False\n",
    "\n",
    "print(\"Training QTCR-Net (demo with 5 epochs)...\")\n",
    "print(\"For full training, use: python train.py --config config.yaml\\n\")\n",
    "\n",
    "trainer = QTCRNetTrainer(config_demo)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Run full training from command line\n",
    "# Uncomment to run full training\n",
    "\n",
    "# !python train.py --config config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Training Progress\n",
    "\n",
    "Load and plot training metrics from TensorBoard logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard logs\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "log_dir = Path(config['training']['logging']['tensorboard_dir'])\n",
    "\n",
    "# Find most recent run\n",
    "runs = sorted(log_dir.glob('*'))\n",
    "if len(runs) > 0:\n",
    "    latest_run = runs[-1]\n",
    "    print(f\"Loading logs from: {latest_run}\")\n",
    "    \n",
    "    # Load event file\n",
    "    event_file = list(latest_run.glob('events.out.tfevents.*'))[0]\n",
    "    ea = event_accumulator.EventAccumulator(str(event_file))\n",
    "    ea.Reload()\n",
    "    \n",
    "    # Extract metrics\n",
    "    train_loss = ea.Scalars('Epoch/Train_Loss')\n",
    "    val_loss = ea.Scalars('Epoch/Val_Loss')\n",
    "    train_wave_acc = ea.Scalars('Epoch/Train_Waveform_Acc')\n",
    "    val_wave_acc = ea.Scalars('Epoch/Val_Waveform_Acc')\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot([x.step for x in train_loss], [x.value for x in train_loss], label='Train', linewidth=2)\n",
    "    axes[0].plot([x.step for x in val_loss], [x.value for x in val_loss], label='Val', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training and Validation Loss', fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot([x.step for x in train_wave_acc], [x.value for x in train_wave_acc], label='Train', linewidth=2)\n",
    "    axes[1].plot([x.step for x in val_wave_acc], [x.value for x in val_wave_acc], label='Val', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('Waveform Classification Accuracy', fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training logs found. Train the model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate Trained Model\n",
    "\n",
    "Load the best checkpoint and evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import QTCRNetEvaluator\n",
    "\n",
    "# Load best checkpoint\n",
    "checkpoint_dir = Path(config['training']['checkpoint']['save_dir'])\n",
    "best_checkpoint = checkpoint_dir / 'best_model.pth'\n",
    "\n",
    "if best_checkpoint.exists():\n",
    "    print(f\"Loading best model from: {best_checkpoint}\")\n",
    "    \n",
    "    # Create evaluator\n",
    "    evaluator = QTCRNetEvaluator(config, str(best_checkpoint))\n",
    "    \n",
    "    # Evaluate\n",
    "    results = evaluator.evaluate()\n",
    "    evaluator.results = results\n",
    "    \n",
    "    # Print results\n",
    "    evaluator.print_results(results)\n",
    "else:\n",
    "    print(f\"Checkpoint not found at: {best_checkpoint}\")\n",
    "    print(\"Train the model first using train.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_checkpoint.exists():\n",
    "    # Plot confusion matrices\n",
    "    evaluator.plot_confusion_matrices(save=False)\n",
    "else:\n",
    "    print(\"Train and evaluate the model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Per-Class Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_checkpoint.exists():\n",
    "    # Plot per-class metrics\n",
    "    evaluator.plot_per_class_metrics(save=False)\n",
    "else:\n",
    "    print(\"Train and evaluate the model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Feature Visualization\n",
    "\n",
    "Extract and visualize intermediate feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_checkpoint.exists():\n",
    "    # Load model\n",
    "    model_eval = evaluator.model\n",
    "    model_eval.eval()\n",
    "    \n",
    "    # Get a test sample\n",
    "    test_voxel, test_labels = next(iter(test_loader))\n",
    "    test_voxel = test_voxel[:1].to(device)  # Single sample\n",
    "    \n",
    "    # Extract feature maps\n",
    "    with torch.no_grad():\n",
    "        feature_maps = model_eval.get_feature_maps(test_voxel)\n",
    "    \n",
    "    print(\"Feature map shapes:\")\n",
    "    for name, feat in feature_maps.items():\n",
    "        print(f\"  {name}: {feat.shape}\")\n",
    "    \n",
    "    # Visualize quantum features\n",
    "    quantum_features = feature_maps['quantum'].cpu().numpy()[0]\n",
    "    \n",
    "    plt.figure(figsize=(14, 4))\n",
    "    plt.bar(range(len(quantum_features)), quantum_features)\n",
    "    plt.xlabel('Quantum Feature Index')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Quantum Reservoir Output Features', fontweight='bold')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Train the model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary and Next Steps\n",
    "\n",
    "### QTCR-Net Architecture Summary\n",
    "\n",
    "**Novel Contributions:**\n",
    "1. **Fully Convolutional Feature Extraction**: Unlike traditional MLP-based approaches, QTCR-Net uses pure convolutional layers for spatio-temporal feature extraction, preserving spatial structure.\n",
    "\n",
    "2. **Temporal Convolutional Network (TCN)**: Multi-scale dilated convolutions (1, 2, 4, 8) capture temporal patterns at different timescales, crucial for event-based data.\n",
    "\n",
    "3. **Multi-Reservoir Quantum Architecture**: Instead of one large quantum circuit (prone to barren plateaus), QTCR-Net uses multiple small quantum reservoirs (4-8 qubits each), each processing different feature groups.\n",
    "\n",
    "4. **Quantum Temporal Reservoirs**: Frozen random quantum circuits act as high-dimensional nonlinear feature extractors (reservoir computing paradigm), avoiding trainability issues.\n",
    "\n",
    "5. **Hybrid Quantum-Classical Learning**: Classical TCN learns temporal patterns, quantum reservoirs provide nonlinear transformations, enabling synergistic learning.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Hyperparameter Tuning**: Experiment with different numbers of quantum groups, qubits, and TCN dilations.\n",
    "\n",
    "2. **Fine-tuning Quantum Layers**: Enable partial training of quantum parameters (set `trainable_quantum: true` in config).\n",
    "\n",
    "3. **Comparison Studies**: Compare QTCR-Net against classical CNNs, TCNs, and MLP+FFT baselines.\n",
    "\n",
    "4. **Hardware Deployment**: Test on quantum hardware (IBM Quantum, IonQ) for real quantum advantage.\n",
    "\n",
    "5. **Transfer Learning**: Pre-train on large event-camera datasets and fine-tune on your specific task.\n",
    "\n",
    "6. **Architecture Ablation**: Study the contribution of each component (TCN, quantum reservoirs, number of groups).\n",
    "\n",
    "### Citation\n",
    "\n",
    "If you use QTCR-Net in your research, please cite:\n",
    "\n",
    "```\n",
    "@misc{qtcrnet2025,\n",
    "  title={QTCR-Net: Quantum Temporal Convolutional Reservoir Network for Event-Based Vision},\n",
    "  author={QTCR-Net Research Team},\n",
    "  year={2025},\n",
    "  note={Novel quantum-hybrid architecture for DVS event camera classification}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## End of Demo\n",
    "\n",
    "For questions or issues, please refer to the README.md or contact the research team."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
